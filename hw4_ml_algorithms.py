# -*- coding: utf-8 -*-
"""HW4_ML_Algorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N4hLGZznfYDVb_T3e7L_2-VLcTqCkYb2
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

reviews = pd.read_csv('/content/drive/MyDrive/HW4(Planet Terp ML analysis)/reviews.csv')
professors = pd.read_csv('/content/drive/MyDrive/HW4(Planet Terp ML analysis)/professors.csv')
grades = pd.read_csv('/content/drive/MyDrive/HW4(Planet Terp ML analysis)/grades.csv')
main_table = pd.read_csv('/content/drive/MyDrive/HW4(Planet Terp ML analysis)/main.csv')

main_table.head()

# 1. Load your main table
df = main_table.copy()

# 2. Drop the redundant columns
df_reduced = df.drop(columns=[
    'num_years_taught',
    'avg_review_char_count',
    'graded_count_prof'
])

df_reduced.head()

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from tqdm.auto import tqdm

# Ensure review text is string: fill NaNs with '' then cast everything to str
reviews['review'] = reviews['review'].fillna('').astype(str)  # :contentReference[oaicite:0]{index=0} :contentReference[oaicite:1]{index=1}

# 1. Download & initialize VADER
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()  # :contentReference[oaicite:2]{index=2}

# 2. Batch-process reviews with a tqdm progress bar
texts = reviews['review'].tolist()
batch_size = 100
compound_scores = []

for i in tqdm(range(0, len(texts), batch_size), desc="Scoring reviews"):
    batch = texts[i : i + batch_size]
    # Compute the compound score (−1 to +1) for each review
    batch_scores = [sia.polarity_scores(txt)['compound'] for txt in batch]
    compound_scores.extend(batch_scores)

# 3. Attach the scores back to your DataFrame
reviews['sentiment_compound'] = compound_scores

# 4. Aggregate average sentiment per professor
avg_sent = (
    reviews
    .groupby('professor')['sentiment_compound']
    .mean()
    .rename('avg_sentiment_compound')
    .reset_index()
)

# 5. Merge into your reduced feature table (df_reduced) to create df_final
df_final = (
    df_reduced
    .merge(avg_sent, left_on='name', right_on='professor', how='left')
    .drop(columns='professor')
)

# 6. Inspect the updated feature table
df_final.head()

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Prepare data
X = df_final.drop(columns=['name', 'average_rating'])
y = df_final['average_rating']

# Train-test split (reproducible with random_state)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Helper to train, predict, and evaluate with 10-fold CV
def evaluate_model_10fold(model, X_tr, X_te, y_tr, y_te):
    model.fit(X_tr, y_tr)
    preds = model.predict(X_te)
    rmse = np.sqrt(mean_squared_error(y_te, preds))
    r2   = r2_score(y_te, preds)
    print(f"{model.__class__.__name__} → RMSE: {rmse:.3f}, R²: {r2:.3f}")

    # 10-fold CV RMSE (negative because of scoring convention)
    cv_scores = cross_val_score(
        model,
        X_tr,
        y_tr,
        cv=10,                                        # ← 10 folds now
        scoring='neg_root_mean_squared_error'
    )
    # Convert back to positive RMSE and report mean ± std
    print(f"  10-fold CV RMSE: {(-cv_scores).mean():.3f} ± {cv_scores.std():.3f}\n")

# 1) K-Nearest Neighbors
knn = KNeighborsRegressor(n_neighbors=5)
evaluate_model_10fold(knn, X_train, X_test, y_train, y_test)

# 2) Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)
evaluate_model_10fold(rf, X_train, X_test, y_train, y_test)

# 3) XGBoost
xgb = XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=4,
    random_state=42,
    objective='reg:squarederror'
)
evaluate_model_10fold(xgb, X_train, X_test, y_train, y_test)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

# 1. Split out X and y one more time
X = df_final.drop(columns=['name', 'average_rating'])
y = df_final['average_rating']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Shared settings
cv_folds = 10
scoring = 'neg_root_mean_squared_error'
n_jobs = -1

# 2. KNN grid search
pipe_knn = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsRegressor())
])
grid_knn = {
    'knn__n_neighbors': [5, 7, 9, 11, 13],
    'knn__weights':     ['uniform', 'distance'],
    'knn__p':           [1, 2]
}
gs_knn = GridSearchCV(pipe_knn, grid_knn, cv=cv_folds, scoring=scoring,
                      n_jobs=n_jobs, error_score='raise')
gs_knn.fit(X_train, y_train)
print("Best KNN params:", gs_knn.best_params_)

# 3. Random Forest grid search
pipe_rf = Pipeline([
    ('scaler', StandardScaler()),
    ('rf', RandomForestRegressor(random_state=42))
])
grid_rf = {
    'rf__n_estimators': [100, 200, 300],
    'rf__max_depth':    [None, 5, 10, 20],
    'rf__max_features': ['sqrt', 'log2', None]
}
gs_rf = GridSearchCV(pipe_rf, grid_rf, cv=cv_folds, scoring=scoring,
                     n_jobs=n_jobs, error_score='raise')
gs_rf.fit(X_train, y_train)
print("Best RF params:", gs_rf.best_params_)

# 4. XGBoost grid search
pipe_xgb = Pipeline([
    ('scaler', StandardScaler()),
    ('xgb', XGBRegressor(objective='reg:squarederror',
                         random_state=42))
])
grid_xgb = {
    'xgb__n_estimators':   [100, 200, 300],
    'xgb__learning_rate':  [0.01, 0.1, 0.2],
    'xgb__max_depth':      [3, 5, 7]
}
gs_xgb = GridSearchCV(pipe_xgb, grid_xgb, cv=cv_folds, scoring=scoring,
                      n_jobs=n_jobs, error_score='raise')
gs_xgb.fit(X_train, y_train)
print("Best XGB params:", gs_xgb.best_params_)

"""Best KNN params: {'knn__n_neighbors': 11, 'knn__p': 1, 'knn__weights': 'uniform'}
Best RF params: {'rf__max_depth': 5, 'rf__max_features': None, 'rf__n_estimators': 300}
Best XGB params: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}
"""

# Retrieve the best KNN pipeline and evaluate
best_knn = gs_knn.best_estimator_

pred_knn = best_knn.predict(X_test)
rmse_knn = np.sqrt(mean_squared_error(y_test, pred_knn))
r2_knn   = r2_score(y_test, pred_knn)

print("Tuned KNN Performance:")
print(f"  RMSE: {rmse_knn:.3f}")
print(f"  R²:   {r2_knn:.3f}")

# Retrieve the best RF pipeline and evaluate
best_rf = gs_rf.best_estimator_

pred_rf = best_rf.predict(X_test)
rmse_rf = np.sqrt(mean_squared_error(y_test, pred_rf))
r2_rf   = r2_score(y_test, pred_rf)

print("Tuned Random Forest Performance:")
print(f"  RMSE: {rmse_rf:.3f}")
print(f"  R²:   {r2_rf:.3f}")

# Retrieve the best XGB pipeline and evaluate
best_xgb = gs_xgb.best_estimator_

pred_xgb = best_xgb.predict(X_test)
rmse_xgb = np.sqrt(mean_squared_error(y_test, pred_xgb))
r2_xgb   = r2_score(y_test, pred_xgb)

print("Tuned XGBoost Performance:")
print(f"  RMSE: {rmse_xgb:.3f}")
print(f"  R²:   {r2_xgb:.3f}")

"""So i don't have to run gridsearch again"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# Best KNN from search
pipe_knn = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsRegressor(
        n_neighbors=11,
        p=1,
        weights='uniform'
    ))
])

# Best Random Forest from search
pipe_rf = Pipeline([
    ('scaler', StandardScaler()),
    ('rf', RandomForestRegressor(
        n_estimators=300,
        max_depth=5,
        max_features=None,
        random_state=42
    ))
])

# Best XGBoost from search
pipe_xgb = Pipeline([
    ('scaler', StandardScaler()),
    ('xgb', XGBRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        objective='reg:squarederror',
        random_state=42
    ))
])

# Refit on X_train (very fast)
pipe_knn.fit(X_train, y_train)
pipe_rf.fit(X_train, y_train)
pipe_xgb.fit(X_train, y_train)

# Evaluate
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# KNN
y_pred_knn = pipe_knn.predict(X_test)
rmse_knn = np.sqrt(mean_squared_error(y_test, y_pred_knn))
r2_knn = r2_score(y_test, y_pred_knn)

# Random Forest
y_pred_rf = pipe_rf.predict(X_test)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

# XGBoost
y_pred_xgb = pipe_xgb.predict(X_test)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2_xgb = r2_score(y_test, y_pred_xgb)

# Print
print(f"KNN → RMSE: {rmse_knn:.3f}, R²: {r2_knn:.3f}")
print(f"Random Forest → RMSE: {rmse_rf:.3f}, R²: {r2_rf:.3f}")
print(f"XGBoost → RMSE: {rmse_xgb:.3f}, R²: {r2_xgb:.3f}")

# Get feature importances from Random Forest
rf_importances = pipe_rf.named_steps['rf'].feature_importances_

# Get feature importances from XGBoost
xgb_importances = pipe_xgb.named_steps['xgb'].feature_importances_

# Feature names (get from X_train columns)
feature_names = X_train.columns

# Combine into a table
import pandas as pd

importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Random Forest Importance': rf_importances,
    'XGBoost Importance': xgb_importances
})

# Sort by XGBoost importance
importance_df = importance_df.sort_values(by='XGBoost Importance', ascending=False)

# Show table
print(importance_df)

import seaborn as sns

# Set a clean style
sns.set(style="whitegrid")

# Create the scatterplot
plt.figure(figsize=(8,6))
sns.regplot(
    data=df_final,
    x='avg_sentiment_compound',
    y='average_rating',
    scatter_kws={'alpha':0.5},   # make points slightly transparent
    line_kws={'color':'red'},    # regression line
    lowess=True                  # smoother trend line
)

# Titles and labels
plt.title('Sentiment Score vs. Average Rating', fontsize=16)
plt.xlabel('Average Sentiment Compound Score', fontsize=14)
plt.ylabel('Average Rating', fontsize=14)
plt.ylim(1, 5)  # because ratings are 1-5 stars
plt.xlim(-1, 1) # sentiment scores range from -1 to +1

# Show the plot
plt.tight_layout()
plt.show()

# Set a clean style
sns.set(style="whitegrid")

# Create the histogram
plt.figure(figsize=(6,4))
sns.histplot(
    data=df_final,
    x='average_rating',
    bins=[1, 2, 3, 4, 5, 6],  # bins aligned with 1-5 stars
    kde=False,
    color='lightgreen',
    edgecolor='black'
)

# Titles and labels
plt.title('Distribution of Average Ratings', fontsize=14)
plt.xlabel('Average Rating (1-5 stars)', fontsize=12)
plt.ylabel('Count of Professors', fontsize=12)
plt.xticks([1, 2, 3, 4, 5])  # clean star labels
plt.xlim(1, 5)

# Layout
plt.tight_layout()

# Show plot
plt.show()